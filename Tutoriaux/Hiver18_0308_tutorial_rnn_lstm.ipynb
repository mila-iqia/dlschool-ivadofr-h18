{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IVADO_rnn_lstm.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"6pL7OrdnLVoy","colab_type":"text"},"cell_type":"markdown","source":["# Tutoriel sur les réseaux neuronaux récurrents"]},{"metadata":{"id":"V_tMgB2wTQTF","colab_type":"text"},"cell_type":"markdown","source":["## Module utilitaire"]},{"metadata":{"id":"OBgc1wsCtAGU","colab_type":"text"},"cell_type":"markdown","source":["Premièrement, installons pytorch et certains modules nécessaires pour compléter ce tutoriel. "]},{"metadata":{"id":"kgw2adqtQHAe","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# http://pytorch.org/\n","from os import path\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","\n","accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n","\n","!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n","import torch"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0iF-E5xOQRe_","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":34},"outputId":"b637e468-42ab-4c79-9c15-5b36757a333f","executionInfo":{"status":"ok","timestamp":1520255585890,"user_tz":300,"elapsed":232,"user":{"displayName":"Jean-Philippe Reid","photoUrl":"//lh4.googleusercontent.com/-sbM9m17mhZQ/AAAAAAAAAAI/AAAAAAAAABM/OYuxWlFkvBk/s50-c-k-no/photo.jpg","userId":"100275820612583042123"}}},"cell_type":"code","source":["from random import randint\n","import numpy as np\n","import time\n","\n","import torch.nn as nn\n","import torch.autograd as autograd\n","import torch.utils.data as data_utils\n","from torch.autograd import Variable\n","from torch import optim\n","torch.backends.cudnn.version()\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","cuda = True\n","\n","torch.manual_seed(123)"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f297d2f8190>"]},"metadata":{"tags":[]},"execution_count":2}]},{"metadata":{"id":"WXqWpBdaQm0-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def normalized(x, y):\n","  '''Cette fonction normalise le tenseur 'x'. '''\n","  x = (x - torch.min(x, 0)[0].view(1, x.shape[1])) / ((torch.max(x, 0)[0] - torch.min(x,0)[0]).view(1, x.shape[1]))\n","\n","  return x, x.sum(dim=1)\n","\n","def standardized(x, y):\n","  '''  Cette fonction 'standardize' le tenseur 'x'. '''  \n","  x = (x - torch.mean(x, 0).view(1, x.shape[1])) / torch.std(x, 0).view(1, x.shape[1])\n","  \n","  return x, x.sum(dim=1)\n","\n","def data_set(N, T, interval):\n","    ''' \n","    inputs : \n","    N : nombre de données \n","    T : longueur de la séquence\n","    interval : l'intervalle dans lequel les nombres seront tirés pour générer les séquences. \n","    \n","    outputs: retourne les séquences (xx) et les cibles (yy), en format torch.tensor \n","    '''\n","    \n","    x = (torch.Tensor(N, T).random_(0, 2 * interval[1]) + interval[0])\n","   \n","    return x, x.sum(dim=1)\n","  \n","def print_sequence(x, y):\n","    '''\n","    x : Une séquence particulière, i.e dim(x) = 1 x T\n","    y : Cible liée à cette même séquence, i.e. dim(y) = 1\n","    retourne une série de caractères illustrant la séquence \n","    dans un format convivial. \n","    '''\n","    n = x.shape[0]\n","    for i, x_ in enumerate(x):\n","        if i == 0 : \n","            string=' ' + str(x_)\n","        else: \n","            string=string + ' + '+str(x_)\n","\n","    return string+' = ' + str(y)\n","  \n","def adjust_lr(optimizer, lr0, epoch, total_epochs):\n","    '''\n","    Cette fonction diminue le taux d'apprentissage suivant une fonction      \n","    exponentielle avec le nombre d'époques. \n","\n","    optimizer: e.g. optim.SGD(... )\n","    lr0 : taux d'apprentissage initial     epoch : époque à laquelle la mise à jour est effectuée. \n","    total epochs: nombre d'époques totales\n","\n","    exemple : \n","\n","    new_learning_rate = adjust_lr(optimizer, 0.01, e_, 100)\n","\n","    '''\n","    \n","    lr = lr0 * (0.36 ** (epoch / float(total_epochs)))\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","    return lr\n","\n","def batch_loss(data_batch,model): \n","    '''\n","    Cette fonction calcule le loss d'un jeu de donnée divisé en plusieurs   \n","    batchs. C'est nécessaire pour traiter de grandes quantités de données.     \n","    '''\n","    loss = 0\n","    n = 0\n","    \n","    for batch_idx, (x, y) in enumerate(data_batch):\n","        x, y = model.input_format(x, y)\n","        out = model(x)\n","        loss += model.criterion(out, y)\n","        n += 1\n","    return loss.data[0] / n\n","  \n","  \n","def adjust_fontsize(ax):\n","  '''\n","  Par déformation professionnelle, j'ai le souci de préparer de belles figures. \n","  Cette fonction est un clin d'oeil pour mon ancien superviseur de thèse.\n","  '''\n","  for ax in ax:\n","    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] \n","                 + ax.get_xticklabels() + ax.get_yticklabels()):\n","      item.set_fontsize(14)  \n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"51tYmwJ2qoqU","colab_type":"text"},"cell_type":"markdown","source":["## Objectif"]},{"metadata":{"id":"ZDMnGX4iqoqV","colab_type":"text"},"cell_type":"markdown","source":["L'objectif de ce tutoriel est de construire un modèle capable d'additionner ou soustraire une série de nombres. Ce jeu de données est facile à générer et nous permet de tester la capacité de plusieurs algorithmes tels que,\n","\n","* LSTM\n","* RNN\n","* MLP\n"]},{"metadata":{"id":"jS8xrTCXqoqW","colab_type":"text"},"cell_type":"markdown","source":["Le jeu de données est constitué de séquences de nombre de longueurs *seq_len* , auxquelles une cible est associée correspondant à la somme de chaque séquence. Dans l'exemple ci-dessous, la i$^{eme}$ composante des données est explicitement détaillée (voir la Fig. 1). \n","\n","\\begin{align}   \\mathrm x^{(i)} &= \\left[ 4,-1,15,24\\right], \\mathrm x^{(i)} \\in \\mathbb R^{d_0} \\\\  \\mathrm y^{(i)} &= 42, \\mathrm y^{(i)} \\in \\mathbb R  \\end{align}\n","\n","Il est important de noter que chaque composante du vecteur $\\mathrm x^{(i)}$ peut être de plusieurs dimensions, c'est-à-dire $x^{(i)}_j \\in \\mathbb R^{d_1}$ où $d_1 > 1$. \n","\n"]},{"metadata":{"id":"QzmAxECSqoqX","colab_type":"text"},"cell_type":"markdown","source":["Ce jeu de donné peut servir à entrainer un modèle de réseau de neurones récurrents (RNN), tel que le _long short term memory_(LSTM). Dans ce cas, chaque nombre sera l'entrée d'une couche cachée (_hidden layer_) de dimension $h_d$. \n","\n","Comme la cible est un nombre réel, il est nécessaire de rajouter une couche linéaire au modèle pour \"ajuster\" les dimensions (voir la Fig.2). Cette notion sera expliquée à l'aide d'un exemple détaillé ci-bas. "]},{"metadata":{"id":"qDCPei0QqoqW","colab_type":"text"},"cell_type":"markdown","source":["![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.001.jpeg)\n","\n","Fig.1 : Jeu de données considéré. "]},{"metadata":{"id":"ok4sQtHUqoqY","colab_type":"text"},"cell_type":"markdown","source":["![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.002.jpeg)\n","\n","\n","Fig.2 : Schéma d'un réseau récurrent. "]},{"metadata":{"id":"RCnneiTsqoqY","colab_type":"text"},"cell_type":"markdown","source":["## Réseau de neuronnes récurrents (LSTM)"]},{"metadata":{"id":"IAisYYriqoqZ","colab_type":"text"},"cell_type":"markdown","source":["### Générer une couche LSTM\n","###  __ = nn.LSTM(input_size, hidden_size, num_layers)"]},{"metadata":{"id":"u93VR8oVqoqa","colab_type":"text"},"cell_type":"markdown","source":["Une couche LSTM peut incorporer plusieurs paramètres dont certains régis par le jeu de données (*input_size*), mais aussi d'autres paramètres essentiels pour optimiser la capacité du modèle, tels que *hidden_size*, *num_layers*, etc."]},{"metadata":{"id":"FgCALpkgqoqb","colab_type":"text"},"cell_type":"markdown","source":["### Entrés d'une couche LSTM (*inputs*)\n","### __ = <font color='red'>LSTM</font>(*input*, (h0,c0))"]},{"metadata":{"id":"EXaWaGleqoqb","colab_type":"text"},"cell_type":"markdown","source":["En plus des données (_input_), il est aussi possible d'initialiser les tenseurs h_0, c_0 définis comme les *hidden* et *cell states*,  paramètres essentiels aux LSTMs. \n","\n","__Dans le cas où $h_0$ et $c_0$ ne sont pas définis, le module LSTM utilisera les valeurs par défaut, i.e. 0.__"]},{"metadata":{"id":"JrRFyn_Tqoqc","colab_type":"text"},"cell_type":"markdown","source":["### Donnée d'entrée (*input*)\n","\n","> Indented block\n","\n","\n","__<font color='red'>input</font> =  torch.Tensor(seq_len, batch_size, input_size) __"]},{"metadata":{"id":"RKuc_SyMqoqd","colab_type":"text"},"cell_type":"markdown","source":["Il est nécessaire de réorganiser les données d'entrées (input) selon trois paramètres : \n","\n","* la longueur de la séquence (seq_len)\n","* la grandeur du lot (batch, i.e. batch_size)\n","* les dimensions des entrées (input_size)\n","\n","Note : seq_len et batch_size peuvent être interchangés si *batch_first = 'True'*"]},{"metadata":{"id":"Dh9ca_HMiKuC","colab_type":"text"},"cell_type":"markdown","source":["### Donnée de sortie (*output*)\n","### output = LSTM(*input*, (h0,c0))\n","__<font color='red'>output</font> = torch.tensor(seq_len,batch_size, hidden_size x num_directions) __\n"]},{"metadata":{"id":"HGShtHdArgvw","colab_type":"text"},"cell_type":"markdown","source":["Les dimensions du tenseur \"output\" sont déterminées par: (*seq_len*, *batch*, *hidden_size* $\\times$ *num_directions*). \n","\n","Dans le cas qui nous intéresse, num_directions $ = 1$. "]},{"metadata":{"id":"zvtzfm1qaj92","colab_type":"text"},"cell_type":"markdown","source":["![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.003.jpeg)\n","\n","Fig. 3: http://pytorch.org/docs/master/nn.html?highlight=lstm#torch.nn.LSTM"]},{"metadata":{"id":"9P1NDwZqqoqc","colab_type":"text"},"cell_type":"markdown","source":["![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.004.jpeg)\n","\n","Fig. 4: http://pytorch.org/docs/master/nn.html?highlight=lstm#torch.nn.LSTM"]},{"metadata":{"id":"n-bi9ydcqoqq","colab_type":"text"},"cell_type":"markdown","source":["![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.005.jpeg)\n","\n","Fig.5 : http://pytorch.org/docs/master/nn.html?highlight=lstm#torch.nn.LSTM"]},{"metadata":{"id":"Maj8vgFlyQlo","colab_type":"text"},"cell_type":"markdown","source":["# Exemple"]},{"metadata":{"id":"qOdqVEOp9Zbz","colab_type":"text"},"cell_type":"markdown","source":["Créons un jeu de données $\\bf x$ composés de 100 séquences de 4 nombres entre 0 et 100. Les cibles $\\bf y$ correspondent à la somme de chacune de ces séquences. La fonction *data_set* située dans la section *Module utilitaire* s'occupe de cette tâche fastidieuse. \n","\n"]},{"metadata":{"id":"14lZPQwTQek8","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Création d'un jeu de donnés \n","x,y = data_set(100, 4, [-100, 100])\n","\n","print(x.shape)\n","print(y.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4YwIajCYJ5vx","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["sequence = print_sequence(x[1,:] ,y[1])\n","\n","print(sequence)\n","print('Dimensions des entrées : {} x {}'.format(*x.shape))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jTKwBXDmqoqk","colab_type":"text"},"cell_type":"markdown","source":["__Q1__ : Selon l'exemple ci-haut, quels sont les paramètres d'entrées? On considéra ici qu'un seul lot (*batch*). \n","\n","* batch_size = \n","* seq_len = \n","* input = \n","\n"]},{"metadata":{"id":"poK1aylHJ53Q","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Q1..."],"execution_count":0,"outputs":[]},{"metadata":{"id":"MAJ9tNujqoql","colab_type":"text"},"cell_type":"markdown","source":["__Q2__  : Donnez un exemple où les dimensions d'entrées sont supérieures à un, c.-à-d. *input* $> 1$.\n","\n","..."]},{"metadata":{"id":"z7HNL-qomf35","colab_type":"text"},"cell_type":"markdown","source":["![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.006.jpeg)\n","\n","\n","Fig.6 : Exemple où la dimension des entrées est supérieure à un. "]},{"metadata":{"id":"2Y4AW07Jqoqq","colab_type":"text"},"cell_type":"markdown","source":["__Q3__  : Construisez votre première couche LSTM. Nous vous référons au lien dessous la Fig. 3 pour plus de détails. "]},{"metadata":{"id":"0zyJ2FFBJ57d","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Q3...\n","###############\n","\n","# les paramètres\n","input_size = 1\n","hidden_size = 6\n","num_layers = 1\n","\n","# lstm  = nn.LSTM(...)\n","\n","# \n","# output_0, (hn, cn) = lstm(Variable(x_))\n","\n","# print('Dimensions - output_0: {}'.format(output_0.shape))\n","# print('Dimensions - h_n: {}'.format(hn.shape))\n","# print('Dimensions - c_n: {}'.format(cn.shape))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MX0t3nT98kAn","colab_type":"text"},"cell_type":"markdown","source":["### Réorganiser les dimensions de la sortie (\"<font color='red'>output_0</font>\") "]},{"metadata":{"id":"ZIVvInzN8zqm","colab_type":"text"},"cell_type":"markdown","source":["Tel que mentionné ci-haut, seule la dernière composante du tenseur \"output_0\" sera considérée pour prédire la cible associée à une séquence (voir Fig.2). \n","\n","Exemple: considérons la $41^e$ séquence ainsi que la couche associée. "]},{"metadata":{"id":"w50E_nbBJ5-e","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(x_[41,:,:])\n","print(y_[41])\n","\n","sequence = print_sequence(x[41,:],y[41])\n","print(sequence)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"F9ZExe-iJ6BE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(output_0[41,:,:])\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vat-3CyN9j9y","colab_type":"text"},"cell_type":"markdown","source":["Tel que mentionné ci-haut, seule la quatrième (dernière) composante du tenseur output_0 qui doit être considéré (voir Fig. 2)."]},{"metadata":{"id":"Jt5sipkq97_b","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["output_0[41,-1,:]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sDD1ICcJL_B7","colab_type":"text"},"cell_type":"markdown","source":["Le tenseur $h_n$ regroupe la dernière sortie (*output*) de chaque séquence."]},{"metadata":{"id":"7rh_45NWMbqT","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(hn[:,41,:])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ewppj5yvsVJT","colab_type":"text"},"cell_type":"markdown","source":["__Q4__  : Proposez une stratégie pour transformer le tenseur \"output_0\" de dimensions [100,4,6] à un tenseur de dimensions voulues, c.-à-d. [100,1]. Spécifiquement, quelle opération mathématique permettra d'obtenir les dimensions voulues? \n","   "]},{"metadata":{"id":"BaS5Ku1GJ6Dr","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Q4\n","\n","'''\n","Première partie: générez un tenseur M de dimensions voulues et utilisez \n","la fonction 'torch.matmul' pour  multiplier M et output_0. \n","\n","Indice : Le tenseur M doit être une 'Variable' pour être multiplié au tenseur \n","output_0\n","\n","M = Variable(torch.Tensor(...))\n","output = torch.matmul(output_0[...], M)\n","\n","'''\n","\n","# ... ici \n","\n","'''\n","Deuxième partie: gérérez une couche linéaire de dimensions voulues avec la \n","fonction 'LL = nn.Linear(_?_, _?_, _?_)'. \n","\n","L'entrée de LL sera output_0, i.e. output = LL(output_0[....])\n","''' \n","\n","# ... ici "],"execution_count":0,"outputs":[]},{"metadata":{"id":"dnWwn0qbAjQG","colab_type":"text"},"cell_type":"markdown","source":["### Construction du modèle"]},{"metadata":{"id":"af7055DXtEq9","colab_type":"text"},"cell_type":"markdown","source":["Il est fortement suggéré d'utiliser des classes pour définir les modèles dans l'environnement Pytorch, et ce peu importe son architecture. \n"]},{"metadata":{"id":"zjTN6N9pB96s","colab_type":"text"},"cell_type":"markdown","source":["__Q5__  : Écrivez la fonction RnnLinear.forward(). Cette fonction aura comme entrée *self* et le tenseur $\\bf x$ de dimension N x T x 1. \n","\n","Indice : le output_0 de la couche LSTM ou RNN sera l'entrée de la couche linéaire. \n","\n","\n"]},{"metadata":{"id":"YRLdrWvNJ6Kz","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class RnnLinear(nn.Module):\n","    def __init__(self,input_size, hidden_size, num_layers, lstm_or_rnn,cuda):\n","        '''\n","        Les paramètres importants du modèle sont définies dans cette fonction. \n","        '''\n","        num_directions = 1\n","        super(RnnLinear, self).__init__()\n","        \n","        # parametres importants \n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm_or_rnn = lstm_or_rnn      \n","        \n","        # couches de notre réseau \n","        if lstm_or_rnn == 'lstm':\n","            self.lstm = nn.LSTM(input_size, hidden_size, \n","                                num_layers, dropout = 0.5,\n","                                batch_first = True)\n","            if cuda : self.lstm.cuda() \n","              \n","        elif lstm_or_rnn == 'rnn':\n","            self.rnn = nn.RNN(input_size, hidden_size,\n","                              num_layers, dropout = 0.5, \n","                              batch_first = True\n","                             )\n","            if cuda: self.rnn.cuda()\n","              \n","        else: print('You made a mistake, pal!')\n","        \n","        self.linear = nn.Linear(hidden_size, input_size, bias=True)\n","        if cuda: self.linear.cuda()\n","              \n","        # criteria for SGD\n","        self.criterion = nn.MSELoss()\n","        \n","    def input_format(self, x, y):\n","        '''\n","        Cette fonction permet de préparer les dimensions des données d'entrées. \n","        '''\n","        x = x[:, :, np.newaxis]\n","        y = y[:, np.newaxis].type(torch.FloatTensor)\n","        \n","        if cuda : \n","            x = Variable(x.cuda())\n","            y = Variable(y.cuda())\n","            \n","        else: \n","            x = Variable(x)\n","            y = Variable(y)\n","        return x, y\n","\n","    def grad_norm(self):\n","        total_norm = 0\n","        for p in list(self.parameters()):\n","            param_norm = p.grad.data.norm(2)\n","            total_norm += param_norm\n","\n","        return total_norm\n","  \n","    def forward(self, x):\n","        '''\n","        Cette fonction devrait inclure des conditions if/elif sur la variable \n","        self.lstm_or_rnn. La sortie des couches LSTM et RNN seront les entrées \n","        de la couche linéaire. \n","        '''\n","        # Q5...\n","        # cas où self.lstm_or_rnn = 'rnn'\n","        # out1 = ...\n","        \n","        # cas où self.lstm_or_rnn = 'lstm'      \n","        # out1 = ...\n","        \n","        # votre couche linéraire             \n","        # out2 = ... \n","        \n","        \n","        return out2"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DFBy2S82DCS2","colab_type":"text"},"cell_type":"markdown","source":["Testons notre modèle!\n","\n","Prenez le temps de bien apprécier les dimensions affichées ci-bas. "]},{"metadata":{"id":"ee-kXlLaDKKV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["x,y = data_set(100, 4, [0,100])\n","\n","\n","input_size = 1\n","num_layers = 2\n","hidden_size = 40 \n","\n","model = RnnLinear(input_size, hidden_size, num_layers, 'lstm', cuda)\n","\n","xx,yy = model.input_format(x, y)\n","print('Dimensions initiales des données : {}'.format(x.shape))\n","print('Dimensions des données formatées : {}'.format(xx.shape))\n","\n","pred = model(xx)\n","print('Dimensions des prédictions : {}'.format(pred.shape))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SMsFy8V1B0vk","colab_type":"text"},"cell_type":"markdown","source":["### Questions\n","\n","__Q6__\n","\n","Construisez la boucle d'entrainement ci-bas. \n","\n","__Q7__\n","\n","Entrainez le modèle sur 20 époques avec les paramètres et hyperparamètres suivants:\n","  * $N_{train}$ = 20000\n","  * T = 10, intervalle = [0,1000]\n","  * $h_d$ = 20\n","  * num_layers = 2\n","  * lr0 = 0.05. \n","\n","__Q8__ : \n","\n","Testez le modèle sur l'ensemble de test. Commentez les résultats obtenus.  \n","\n","Note: L1 loss est définie comme la somme des différences absolue entre les cibles et les prédictions. Pour l'ensemble de test considéré, cette quantité est définie comme \\begin{align}\n","L1 = \\sum_{i=1}^{5000} \\mid y^{(i)} - \\text{output}^{(i)}\\mid \\end{align}\n","\n","__Q9__ \n","\n","Augmentez la capacité du modèle, en fixant $h_d$ = 40. Comparez les résultats à ceux précédents.\n","\n","__Q10__ \n","\n","*'Standardisez'* les données de chaque séquence. Vous devrez utiliser la fonction *x, y = standardized(x, y)* définie dans le module utilitaire. \n","\n","De plus, élargissez l'intervalle de tirage de -1000 à 1000 et entrainez jusqu'à 100 époques. Comparez les résultats à ceux précédents.\n","\n","__Q11__\n","\n","Il est possible d'activer ou de désactiver l'option dropout du modèle en appelant 'model.train()' et 'model.eval()' respectivement. Testez ces deux modes. \n","\n","Pourquoi est-il nécessaire de désactiver l'option 'dropout' lors des phases de test et de validation? \n","\n","__Q12__ \n","\n","Explorez différents hyperparamètres. \n","\n","* lr = 0.1, 1, 0.00001\n","\n","* hidden_size = 100,150, 200\n","\n","* dropout = 0.1, 0.3, 0.5\n","\n","Enjoy! \n","\n"]},{"metadata":{"id":"tnV7bUgZsvz7","colab_type":"text"},"cell_type":"markdown","source":["### Création du jeu de données"]},{"metadata":{"id":"c8AsbJZmJ6OB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["x,y = data_set(20000, 10, [0,1000])\n","\n","xtrain, ytrain = (x[:10000], y[:10000])\n","xvalid, yvalid = (x[10000:15000], y[10000:15000])\n","xtest, ytest = (x[15000:], y[15000:])\n","\n","batch_size = 100\n","\n","all_data_train = data_utils.DataLoader(data_utils.TensorDataset(xtrain, ytrain),\n","                                       batch_size, shuffle=True)\n","all_data_valid = data_utils.DataLoader(data_utils.TensorDataset(xvalid, yvalid), \n","                                       batch_size, shuffle=False)\n","all_data_test = data_utils.DataLoader(data_utils.TensorDataset(xtest, ytest),\n","                                       batch_size, shuffle=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hxVLLPwMFh0o","colab_type":"text"},"cell_type":"markdown","source":["### Entraînement du modèle"]},{"metadata":{"id":"srcOUXeqQrZl","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["input_size = 1\n","num_layers = 2\n","hidden_size = 20\n","number_epoch = 20\n","lr0 = 0.05\n","\n","model = RnnLinear(input_size, hidden_size, num_layers, 'lstm', cuda)\n","optimizer = optim.SGD(model.parameters(), lr=lr0)\n","\n","t0 = time.clock()\n","\n","# tableau à remplir lors de la phase d'entrainement \n","loss_train_data = np.array([]) \n","loss_valid_data = np.array([]) \n","\n","total_norm = np.array([])\n","\n","t0 = time.clock()  \n","\n","for e_ in range(number_epoch):\n","  # model.train(), poruquoi? Indice : dropout   \n","  model.train()\n","  for batch_idx, (xx, yy) in enumerate(all_data_train):                \n","    # Q6...\n","    # Formatez les données. Indice : model.input_format(...)\n","    #     ici...\n","\n","    # Évaluez le modèle \n","    #     ici...\n","    \n","    # Initialisez les gradients\n","    #     ici...\n","    \n","    # Calculez la fonction de coût de la batch\n","    #     ici...\n","    \n","    # SGD backward \n","    #     ici...\n","    \n","    # SGD optimizer\n","    #     ici...\n","    \n","    \n","    # stop. \n","  \n","  # model.grad_norm(): calcule la norm de tous les gradients. \n","  total_norm = np.append(total_norm,model.grad_norm())\n","\n","  # model.eval(), pourquoi? Indice : dropout \n","  model.eval()  \n","\n","  loss_train_data = np.append(loss_train_data, batch_loss(all_data_train,model)) \n","  loss_valid_data = np.append(loss_valid_data, batch_loss(all_data_valid,model))\n","\n","  if e_%10 == 0: \n","    # '{} {}'.format('foo', 'bar')\n","    print('N. of Epochs = {}, Train Loss = {}, Valid Loss = {}'.format(e_, loss_train_data[e_], loss_valid_data[e_]))\n","\n","\n","  lr_ = adjust_lr(optimizer, lr0, e_, number_epoch)\n","\n","tf = time.clock()\n","print('Terminé, {} sec'.format(tf-t0))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"16ZBNnYKFJCS","colab_type":"text"},"cell_type":"markdown","source":["### Prédiction du modèle"]},{"metadata":{"id":"_r19roAVGeSk","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# prêt pour évaluer\n","model.eval()\n","\n","# prêt pour entraîner\n","# model.train()\n","\n","xtest_,ytest_ = model.input_format(xtest, ytest)\n","pred_test_all = model(xtest_)\n","\n","L1_loss = torch.sum(torch.abs(pred_test_all - ytest_)).data[0]\n","\n","print('LSTM L1 loss = {}'.format(L1_loss))\n","print('      ')\n","print('Prediction')\n","print('___________________')\n","print(pred_test_all[0:10])\n","print('      ')\n","print('Target')\n","print('___________________')\n","print(ytest_[:10])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7yLEDkGpD_Zj","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"lfQB8XPdF8wO","colab_type":"text"},"cell_type":"markdown","source":["### Courbes d'apprentissage"]},{"metadata":{"id":"zKcZTByAxCjk","colab_type":"text"},"cell_type":"markdown","source":["Nous traçons plus bas deux importants graphiques: \n","\n","* Courbe d'apprentissage, i.e. loss vs. époque pour l'ensemble de validation et d'entrainement. \n","* La norme de tous les gradients évaluée à chaque époque. \n"]},{"metadata":{"id":"dygg3CwaKwxw","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["fig, (ax1,ax2) = plt.subplots(1,2, figsize=(15,6))\n","ax1.plot(loss_train_data,'-k', label='Train')\n","ax1.plot(loss_valid_data,'-r', label='Valid')\n","\n","ax2.plot(total_norm, '-b', label='Grad')\n","\n","\n","ax1.set_xlabel('epoch', fontsize=14)\n","ax2.set_xlabel('epoch', fontsize=14)\n","ax1.set_ylabel('MSE loss', fontsize=14)\t\n","ax2.set_ylabel('Grad Norm', fontsize=14)\t\n","legend = ax1.legend(loc='upper right', fontsize=14)\n","\n","ax1.set_xlim((0))\n","\n","ax1.set_title('Learning curve', fontsize=14)\n","ax1.set_title('Learning curve', fontsize=14)\n","\n","xmin, xmax = ax2.get_xlim()\n","ymin, ymax = ax2.get_ylim()\n","\n","ax2.text(0.6*(xmax - xmin) + xmin, 0.8 * (ymax-ymin) + ymin,\n","         'num_layers : '+str(num_layers), fontsize=14)\n","\n","ax2.text(0.6*(xmax - xmin) + xmin, 0.72 * (ymax-ymin) + ymin,\n","         'hidden_size : '+str(hidden_size), fontsize=14)\n","\n","ax2.text(0.6*(xmax - xmin) + xmin, 0.64 * (ymax-ymin) + ymin,\n","         'number_epoch : '+str(number_epoch), fontsize=14)\n","\n","ax2.text(0.6*(xmax - xmin) + xmin, 0.56 * (ymax-ymin) + ymin,\n","         'lr$_0$ : '+str(lr0), fontsize=14)\n","\n","\n","adjust_fontsize([ax1, ax2])\n","plt.show()          \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uXLLEtnrGBdn","colab_type":"text"},"cell_type":"markdown","source":["### Question\n","\n","__Q13__ : Dans le graphique ci-haut, comment expliquez-vous que la norme diminue et converge vers zéro avec le nombre d'époques? \n","\n","..."]},{"metadata":{"id":"iqPYtKa0E4UR","colab_type":"text"},"cell_type":"markdown","source":["## Prédiction sur des séquences de longueurs différentes"]},{"metadata":{"id":"-vKX2NMyF5oj","colab_type":"text"},"cell_type":"markdown","source":["### Question"]},{"metadata":{"id":"EA9MAlcqaNae","colab_type":"text"},"cell_type":"markdown","source":["__Q14__ : Prouvez qu'il est aussi possible d'utiliser ce modèle pour prédire des séquences d'une longueur différente."]},{"metadata":{"id":"xr5OPW0W7lnd","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Q14..."],"execution_count":0,"outputs":[]},{"metadata":{"id":"LJ1XwmGVKGVa","colab_type":"text"},"cell_type":"markdown","source":["# Comparaison entre RNN et LSTM"]},{"metadata":{"id":"YMXnblC77hwv","colab_type":"text"},"cell_type":"markdown","source":["__Q15__ :  Entrainez le modèle ci-bas pour les hyperparamètres suivants:\n","\n","* N = 20000\n","* $h_d$ = 20, 50, 100, 150, 200\n","* number_epoch = 1\n","* batch_size = 100\n","* lr0 = 0.005\n","* T = 50, 100, 150, 200\n","\n","Commentez les résultats obtenus. \n","\n","\n","__Q16__ : Entrainez le modèle ci-bas pour ces séries d'hyperparamètres et comparez les performances des modèles RNN et LSTM. \n","\n","Nous vous suggérons de remplacer 'if epoch%1 == 0: ' par 'if epoch%10 == 0: '\n","\n","* N = 20000\n","* $h_d$ = 50\n","* number_epoch = 100\n","* batch_size = 100\n","* lr0 = 0.005\n","\n","  * T = 20, 50, 100, 200\n","  \n","\n","\n","\n","\n"]},{"metadata":{"id":"BXM6MKtmKw7v","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Q15 et Q16\n","# ici ..."],"execution_count":0,"outputs":[]},{"metadata":{"id":"cdmCKBO9jR_d","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["fig, ((ax1, ax2),(ax3, ax4)) = plt.subplots(2, 2, figsize=(16,12))\n","ax1.plot(loss_train_data_rnn, '-k', label ='train')\n","ax1.plot(loss_valid_data_rnn, '-r', label = 'valid')\n","\n","ax2.plot(total_norm_rnn, '-k', label = 'norm rnn')\n","\n","ax3.plot(loss_train_data_lstm, '-k', label='train')\n","ax3.plot(loss_valid_data_lstm, '-r', label='valid')\n","\n","ax4.plot(total_norm_lstm, '-k', label='norm lstm')\n","\n","ax3.set_xlabel('epoch', fontsize=14)\n","ax4.set_xlabel('epoch', fontsize=14)\n","\n","ax1.set_ylabel('MSE loss', fontsize=14)\t\n","ax3.set_ylabel('MSE loss', fontsize=14)\t\n","\n","ax1.set_title('RNN', fontsize=14)\n","ax2.set_title('RNN', fontsize=14)\n","ax3.set_title('LSTM', fontsize=14)\n","ax4.set_title('LSTM', fontsize=14)\n","legend = ax1.legend(loc='upper right', fontsize=14)\n","legend = ax3.legend(loc='upper right', fontsize=14)\n","\n","\n","ax1.set_xlim((0, number_epoch))\n","ax1.set_ylim((0))\n","ax2.set_ylim((0))\n","ax3.set_xlim((0, number_epoch))\n","ax3.set_ylim((0))\n","ax4.set_ylim((0))\n","\n","xmin, xmax = ax1.get_xlim()\n","ymin, ymax = ax1.get_ylim()\n","\n","ax1.text(0.1*(xmax - xmin) + xmin, 0.6*(ymax-ymin) + ymin,\n","         'num_layers : %.f'%num_layers, fontsize=14)\n","ax1.text(0.1*(xmax - xmin) + xmin, 0.52*(ymax-ymin) + ymin,\n","         'hidden_size : %.f'%hidden_size, fontsize=14)\n","ax1.text(0.1*(xmax - xmin) + xmin, 0.44*(ymax-ymin) + ymin,\n","         'number_epoch : %.f'%number_epoch, fontsize=14)\n","ax1.text(0.1*(xmax - xmin) + xmin, 0.36*(ymax-ymin) + ymin,\n","         'lr$_0$ : %.1f'%lr0, fontsize=14)\n","\n","\n","for ax in [ax1, ax2, ax3, ax4]:\n","  for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] \n","               + ax.get_xticklabels() + ax.get_yticklabels()):\n","    item.set_fontsize(14)\n","\n","plt.show()          \n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"63mTDEWcT5ZK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"nF8hxt0NugZP","colab_type":"text"},"cell_type":"markdown","source":["# MLP\n","\n","Pour la suite, nous vous proposons d'intégrer ce jeu de données à un MLP, tel que présenté par Arsène Fansi Tchango.\n","\n","Voici les données : \n"]},{"metadata":{"id":"B-5DXS_iQ3jp","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["x,y = data_set(20000, 10, [-1000,1000])\n","\n","x = torch.FloatTensor(x)\n","y = torch.FloatTensor(y).long()\n","\n","xtrain, ytrain = standardized(x[:10000], y[:10000])\n","xvalid, yvalid = standardized(x[10000:15000], y[10000:15000])\n","xtest, ytest = standardized(x[15000:], y[15000:])\n","\n","batch_size = 100\n","\n","all_data_train = data_utils.DataLoader(data_utils.TensorDataset(xtrain, ytrain),\n","                                       batch_size, shuffle=True)\n","all_data_valid = data_utils.DataLoader(data_utils.TensorDataset(xvalid, yvalid), \n","                                       batch_size, shuffle=False)\n","all_data_test = data_utils.DataLoader(data_utils.TensorDataset(xtest, ytest),\n","                                       batch_size, shuffle=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"e_imza-4H0oO","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# MLP... "],"execution_count":0,"outputs":[]},{"metadata":{"id":"vOw9EYwfIGxJ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Training..."],"execution_count":0,"outputs":[]},{"metadata":{"id":"VnEKQwyNKcTg","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["xtest_, ytest_ = model.input_shape(xtest, ytest)\n","pred_test_all = model(xtest_).view(xtest_.shape[0])\n","\n","\n","L1_loss = torch.sum(torch.abs(pred_test_all - ytest_)).data[0]\n","\n","print('LSTM L1 loss = {}'.format(L1_loss))\n","print('      ')\n","print('Prediction')\n","print('___________________')\n","print(pred_test_all[:10])\n","print('      ')\n","print('Target')\n","print('___________________')\n","print(ytest_[:10])\n","\n"],"execution_count":0,"outputs":[]}]}